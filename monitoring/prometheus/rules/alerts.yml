groups:
  - name: iot-api-alerts
    rules:
      # Alertes sur l'API
      - alert: APIDown
        expr: up{job=~"iot-api-.*"} == 0
        for: 30s
        labels:
          severity: critical
          service: iot-api
        annotations:
          summary: "API IoT est indisponible"
          description: "L'API IoT {{ $labels.instance }} est indisponible depuis {{ $value }} secondes"

      - alert: APIHighLatency
        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket{job=~"iot-api-.*"}) > 1
        for: 2m
        labels:
          severity: warning
          service: iot-api
        annotations:
          summary: "Latence élevée de l'API"
          description: "95% des requêtes prennent plus de 1 seconde sur {{ $labels.instance }}"

      - alert: APIHighErrorRate
        expr: rate(http_requests_total{job=~"iot-api-.*",status=~"5.."}[5m]) / rate(http_requests_total{job=~"iot-api-.*"}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          service: iot-api
        annotations:
          summary: "Taux d'erreur élevé de l'API"
          description: "Plus de 5% d'erreurs 5xx sur {{ $labels.instance }} ({{ $value | humanizePercentage }})"

      - alert: APILowThroughput
        expr: rate(http_requests_total{job=~"iot-api-.*"}[5m]) < 0.1
        for: 10m
        labels:
          severity: info
          service: iot-api
        annotations:
          summary: "Faible trafic sur l'API"
          description: "Moins de 0.1 requête/seconde sur {{ $labels.instance }}"

  - name: infrastructure-alerts
    rules:
      # Alertes système
      - alert: HighCPUUsage
        expr: 100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "Utilisation CPU élevée"
          description: "Utilisation CPU > 80% sur {{ $labels.instance }} ({{ $value }}%)"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "Utilisation mémoire élevée"
          description: "Utilisation mémoire > 85% sur {{ $labels.instance }} ({{ $value }}%)"

      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
        for: 5m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "Espace disque faible"
          description: "Moins de 15% d'espace disque disponible sur {{ $labels.instance }} ({{ $value }}%)"

      - alert: HighDiskIOWait
        expr: rate(node_cpu_seconds_total{mode="iowait"}[5m]) * 100 > 20
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "Attente I/O disque élevée"
          description: "I/O wait > 20% sur {{ $labels.instance }} ({{ $value }}%)"

  - name: database-alerts
    rules:
      # Alertes base de données
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 30s
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL indisponible"
          description: "PostgreSQL est indisponible sur {{ $labels.instance }}"

      - alert: PostgreSQLHighConnections
        expr: (pg_stat_database_numbackends / pg_settings_max_connections) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Nombreuses connexions PostgreSQL"
          description: "Plus de 80% des connexions utilisées sur {{ $labels.instance }} ({{ $value }}%)"

      - alert: PostgreSQLHighQueryTime
        expr: pg_stat_activity_max_tx_duration > 300
        for: 2m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Requête PostgreSQL longue"
          description: "Requête en cours depuis plus de 5 minutes sur {{ $labels.instance }}"

      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag > 60
        for: 1m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Retard de réplication PostgreSQL"
          description: "Retard de réplication > 60 secondes sur {{ $labels.instance }}"

  - name: containers-alerts
    rules:
      # Alertes conteneurs
      - alert: ContainerDown
        expr: up{job="cadvisor"} == 0
        for: 30s
        labels:
          severity: warning
          component: containers
        annotations:
          summary: "cAdvisor indisponible"
          description: "Impossible de collecter les métriques conteneurs depuis {{ $labels.instance }}"

      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{name!=""}[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: containers
        annotations:
          summary: "Conteneur avec CPU élevé"
          description: "Conteneur {{ $labels.name }} utilise plus de 80% CPU"

      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""}) * 100 > 90
        for: 5m
        labels:
          severity: warning
          component: containers
        annotations:
          summary: "Conteneur avec mémoire élevée"
          description: "Conteneur {{ $labels.name }} utilise plus de 90% de sa limite mémoire"

      - alert: ContainerRestartFrequent
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 0m
        labels:
          severity: warning
          component: containers
        annotations:
          summary: "Redémarrages fréquents de conteneur"
          description: "Conteneur {{ $labels.container }} redémarre fréquemment"

  - name: connectivity-alerts
    rules:
      # Alertes de connectivité
      - alert: HTTPEndpointDown
        expr: probe_success{job="blackbox-http"} == 0
        for: 30s
        labels:
          severity: critical
          component: connectivity
        annotations:
          summary: "Endpoint HTTP indisponible"
          description: "{{ $labels.instance }} ne répond pas aux requêtes HTTP"

      - alert: HTTPSlowResponse
        expr: probe_duration_seconds{job="blackbox-http"} > 5
        for: 5m
        labels:
          severity: warning
          component: connectivity
        annotations:
          summary: "Réponse HTTP lente"
          description: "{{ $labels.instance }} répond en plus de 5 secondes ({{ $value }}s)"

      - alert: ICMPDown
        expr: probe_success{job="blackbox-icmp"} == 0
        for: 1m
        labels:
          severity: warning
          component: connectivity
        annotations:
          summary: "Host non joignable en ICMP"
          description: "{{ $labels.instance }} ne répond pas au ping"

      - alert: SSLCertExpiringSoon
        expr: (probe_ssl_earliest_cert_expiry - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
          component: ssl
        annotations:
          summary: "Certificat SSL expire bientôt"
          description: "Le certificat SSL de {{ $labels.instance }} expire dans {{ $value }} jours"

      - alert: SSLCertExpired
        expr: (probe_ssl_earliest_cert_expiry - time()) / 86400 < 0
        for: 1m
        labels:
          severity: critical
          component: ssl
        annotations:
          summary: "Certificat SSL expiré"
          description: "Le certificat SSL de {{ $labels.instance }} a expiré"

  - name: monitoring-alerts
    rules:
      # Alertes sur le monitoring lui-même
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus indisponible"
          description: "Le serveur Prometheus est indisponible"

      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Échec du rechargement de configuration Prometheus"
          description: "Le rechargement de la configuration Prometheus a échoué"

      - alert: AlertManagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "AlertManager indisponible"
          description: "AlertManager est indisponible"

      - alert: TooManyAlerts
        expr: sum(ALERTS{alertstate="firing"}) > 20
        for: 5m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Trop d'alertes actives"
          description: "Plus de 20 alertes sont actuellement actives ({{ $value }})"